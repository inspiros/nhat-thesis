%!TEX root = ../../main.tex

\subsection{Algorithm to solve pc-MvDA}
    As our objective does not match any optimization form of generalized eigenvalue problems, we use gradient descent algorithm to solve pc-MvDA. This approach also allow the algorithm to update incrementally even in higher dimensional data where the computations of matrix inversion and eigenvectors are not conducive. The derivative of Eq.\eqref{eq:pc-MvDA} is:

    \begin{equation}
        \frac{\partial J\left(W\right)}{\partial W}=-\sum_{a<b}^{c}{\frac{qn_an_b}{{n_{cc}}^2{J_{ab}\left(W\right)}^{q+1}}\frac{\partial J_{ab}\left(W\right)}{\partial W}}
    \end{equation}
    where $J_{ab}\left(W\right)={tr\left({\boldsymbol{S}_B^y}_{ab}\right)}/{tr\left({\boldsymbol{S}_W^y}_{ab}\right)}$ is the Fisher loss of class pair $a$ and $b$. Its gradient is computed using the funky trace derivative and quotient rule:
    \begin{equation}
        \frac{\partial J_{ab}\left(W\right)}{\partial W}=\frac{tr\left({\boldsymbol{S}_W^y}_{ab}\right)W^T\left({\boldsymbol{S}_B^x}_{ab}+{{\boldsymbol{S}_B^x}_{ab}}^T\right)-tr\left({\boldsymbol{S}_B^y}_{ab}\right)W^T\left({\boldsymbol{S}_W^x}_{ab}+{{\boldsymbol{S}_W^x}_{ab}}^T\right)}{{tr\left({\boldsymbol{S}_W^y}_{ab}\right)}^2}
        \label{eq:grad_Jab}
    \end{equation}
    The superscript $x$ replacing $y$ in scatter matrices notations means that they denote the pre-transformed version. With simple algebra, we can rewrite the formulas in matrix multiplication form, separating transformation vectors $\omega_j$ in a concatenated matrix $W$ and a multi-view covariance matrix constructed from $v\times v$ cells, each cell $S_{jk}$ stands for the covariance between view $j$ and view $k$.
    \begin{equation}
        \boldsymbol{S}^y=W^T\boldsymbol{S}^xW=\left[\begin{matrix}\omega_1^T&\omega_2^T&\cdots&\omega_v^T\\\end{matrix}\right]\left[\begin{matrix}S_{11}&S_{12}&\cdots&S_{1v}\\S_{21}&S_{22}&\cdots&S_{2v}\\\vdots&\vdots&\ddots&\vdots\\S_{v1}&S_{v2}&\cdots&S_{vv}\\\end{matrix}\right]\left[\begin{matrix}\omega_1\\\omega_2\\\vdots\\\omega_v\\\end{matrix}\right]
    \end{equation}

    The complete algorithm to compute $\nabla J$ is given in Algorithm \ref{algo:grad_computation}.

    \begin{algorithm}
        \SetKwInOut{Input}{Input}
        \SetKwInOut{Output}{Output}
        \SetEndCharOfAlgoLine{\relax}
        \Input{$W$, \{$X$, $\mu$\}, $q$}
        \Output{$\nabla J\left(W\right)$}
        $\boldsymbol{F} = 0$\;
        Compute $\boldsymbol{S}_W^x$ according to Eq.\eqref{eq:MvDA_Sw}\;
        \For{$a=1$ \KwTo $c$} {
            \For{$b=a+1$ \KwTo $c$} {
                Compute ${\boldsymbol{S}_B^x}_{ab}$ according to Eq.\eqref{eq:Sb_ab}\;
                Compute ${\boldsymbol{S}_W^x}_{ab}$ according to Eq.\eqref{eq:Sw_ab}\;
                Compute ${\boldsymbol{S}_B^y}_{ab}=W^T{\boldsymbol{S}_B^x}_{ab}W$\;
                Compute ${\boldsymbol{S}_W^y}_{ab}=W^T{\boldsymbol{S}_W^x}_{ab}W$\;
                Compute $J_{ab}=tr\left({\boldsymbol{S}_B^y}_{ab}\right)/tr\left({\boldsymbol{S}_W^y}_{ab}\right)$\;
                Compute $\nabla J_{ab}$ according to Eq.\eqref{eq:grad_Jab}\;
                $\boldsymbol{F} = \boldsymbol{F} + n_an_b\nabla J_{ab}/J_{ab}^{q+1}$\;
            }
        }
        $\nabla J = {-q\boldsymbol{F}}/{n_{cc}^2}$\;
        \Output{$\nabla J$}
        \caption{Computation of $\nabla J\left(W\right)$ (i.e. gradient of Eq.\eqref{eq:pc-MvDA})}
        \label{algo:grad_computation}
    \end{algorithm}

    We do not impose any further constraint to the model. In our implementation of pc-MvDA in Pytorch, the computation of gradient is in fact handled by automatic differentiation. We choose Adam as our optimizer with learning rate set to $0.01$. To have better starting point and mitigate variances, we initialize pc-MvDA with transformation learnt by MvDA.