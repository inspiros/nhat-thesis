% !TEX root = ../main.tex

\chapter{Introduction} \label{chap:introduction}
    \section{Motivation} \label{sec:intro_motivation}
        Human action and gesture recognition aims at recognizing an action from a given video clip. This is an attractive research topic, which has been extensively studied over the years due to its broad range of applications from video surveillance to human machine interaction \cite{herath2017going, zhang2019comprehensive}.
        %While action and gesture recognition in constrained simple backgrounds scene taken from common views seems to be a well solved problem, recognizing actions from real-world complex scenes has to face new challenges.
        Within this scope, a very important demand is independence to viewpoint. However, different viewpoints result in various human pose, background, camera motions, lighting conditions and occlusions. Consequently, recognition performance could be dramatically degraded under viewpoint changes.

        To overcome this problem, a number of methods have been proposed. View independence recognition such as \cite{gavrila19963, lv2007single, weinland2007action} \cite{weinland2011survey} generally require a careful multi-view camera setting for robust joint estimation. View invariance approach \cite{junejo2008cross, li2012cross} is usually limited by inherent structure of view-invariant features. Recently, knowledge transfer technique is widely deployed for cross-view action recognition, for instance bipartite graph that bridge the semantic gap across view dependent vocabularies \cite{liu2011cross}, AND-OR graph (MSTAOG) for cross-view action recognition \cite{wang2014cross}. To increase discriminant and informative features, view private features and shared features are both incorporated in such frameworks to learn the common latent space \cite{kong2017deeply, liu2018hierarchically}. 
        While existing works for human action and gesture recognition from common viewpoints explored different deep learning techniques and achieved impressive accuracy. In most of aforementioned multi-view action recognition techniques, the features extracted from each view are usually hand-crafted features (i.e improved dense trajectories) \cite{rahmani2017learning, liu2018hierarchically,kong2017deeply}. Deep learning techniques, if used, handle knowledge transfer among viewpoints. Deployment of deep features in such frameworks for cross-view scenario is under investigation. 
        %To overcome this problem, a number of methods have been proposed. View independence recognition has been surveyed in \cite{weinland2011survey}. View normalization 2D/3D attempts to estimate view transformation between model and observation \cite{gavrila19963, lv2007single, weinland2007action}. These approaches generally require a careful multi-view camera setting for robust joint estimation. View invariance approach searched for features and matching functions that are independent with view changes \cite{junejo2008cross, li2012cross}. The performance of this approach is limited by inherent structure of view-invariant features. Recently, knowledge transfer technique is widely deployed for cross-view action recognition. Liu et al. utilized a bipartite graph to build bilingual words that bridge the semantic gap across view dependent vocabularies \cite{liu2011cross}. Wang et al. introduced a multi-view spatio-temporal AND-OR graph (MSTAOG) representation for cross-view action recognition \cite{wang2014cross}. Zhang et al. built continuous virtual path which connects the source view and the target view \cite{zhang2013cross}. To increase discriminant and informative features, view private features and shared features are both incorporated in such frameworks to learn the common latent space \cite{kong2017deeply, liu2018hierarchically}. While existing works for human action and gesture recognition from common viewpoints explored different deep learning techniques and achieved impressive accuracy. In most of aforementioned multi-view action recognition techniques, the features extracted from each view are usually hand-crafted features (i.e improved dense trajectories) \cite{rahmani2017learning, liu2018hierarchically,kong2017deeply}. Deep learning techniques, if used, handle knowledge transfer among viewpoints. Deployment of deep features in such frameworks for cross-view scenario is under investigated. 

        In parallel with knowledge transfer techniques, building a common space from different views has been addressed in many other works using multi-view discriminant analysis techniques.
        The first work of this approach was initiated by Canonical Component Analysis (CCA) that tried to find two linear transformations for each view \cite{thompson1984canonical}.
        Various improvements of CCA have been made to take non-linear transformation into account (kernel CCA) or to extend to multiple viewpoints (Multi-view CCA) \cite{hardoon2004canonical}. %Then, GMA has improved MCCA to preserve supervised structure of each view \cite{yang2014multi, cao2017generalized}. 
        Henceforth, different improvements have been introduced such as MULDA \cite{yang2014multi}, MvDA \cite{kan2015multi}, MvMDA \cite{you2019multi}, MCCDA \cite{you2019multi}.
        All of these techniques try to build a common space from different views by maximizing between-classes while minimizing within-classes distances among views.
        However, most of these works are still experimented with static images, none of them have been explored with videos.
        Particularly, their investigation for the case of human action recognition is still actively under taken.

    \section{Objective} \label{sec:intro_objective}
        The main goal of this thesis is to develop an effective approach for designing a noise robust small footprint phoneme classifier system which is also computationally efficient. 
        This will be accomplished through two contributions. 
        The first contribution is designing a system that has the ability to replace a software or hardware denoiser by including noisy data in the training process of the machine learning technique. 
        This reduces the cost, memory requirement and increases the robustness of the speech engine and the processing speed.
        This is achieved by applying multi-condition training of the recognizer. 
        The targeted noises are represented by injecting the corresponding noise into the training data enabling the model to simulate the noisy environments as it is trained with the clean and noisy data.
        Hence the denoising aspect is no longer a separate module but rather incorporated in the deep learning model itself.
        The second is to reach a good compromise between the clean and noisy data training, replacing the need for multiple noise models with a single one that provides accurate and robust results for both clean and noisy conditions. 
        This also serves the purpose of reducing memory footprint and decreasing the pre-processing delay caused by the model selection step.

    \section{Thesis Outline} \label{sec:intro_outline}
        The thesis is structured into 5 chapters:
        \begin{description}
            \item[1 Introduction] This chapter. Motivates the work and describes the research goals.
            \item[2 Background] Describes the various algorithms for feature extraction, machine learning, and distance calculation used throughout this work. Also explains the general system structure of the developed approaches.
            \item[3 Proposed method] Summarizes existing methods for solving the mentioned research objectives.
            \item[4 Experiments] 
            \item[5 Conclusion] Summarizes the work, points out the major contributions, and suggests future research directions.
        \end{description}
