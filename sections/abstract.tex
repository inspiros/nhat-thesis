%!TEX root = ../main.tex

%\renewcommand{\abstractname}{Kurzfassung}
\begin{abstract}
    % \addcontentsline{toc}{chapter}{Abstract}
    % \chapter*{Abstract}

    Human action recognition (HAR) has many implications in robotic and medical applications.
    Invariance under different viewpoints is one of the most critical requirements for practical deployment as it affects many aspects of the information captured such as occlusion, posture, color, shading, motion and background.
    In this thesis, a novel framework that leverages successful deep features for action representation and multi-view analysis to accomplish robust HAR under viewpoint changes.
    Specifically, various deep learning techniques, from 2D CNNs to 3D CNNs are investigated to capture spatial and temporal characteristics of actions at each individual view.
    A common feature space is then constructed to keep view invariant features among extracted streams.
    This is carried out by learning a set of linear transformations that projects separated view features into a common dimension.
    To this end, Multi-view Discriminant Analysis (MvDA) is adopted.
    Then, I introduce pairwise-covariance maximizing extension that takes extra-class discriminance into account, namely pc-MvDA.
    Experimental results on three datasets (IXMAS, MuHAVI, MICAGes) show the effectiveness of proposed method.


    % Human action recognition (HAR) under different viewpoints is one of the most critical requirement for practical deployment.
    % In this thesis, I propose a novel method that leverages successful deep features for action representation and multi-view discriminant analysis to make robust HAR under viewpoint change.
    % Specifically, various deep learning techniques, from 2D CNNs to 3D CNNs are investigated to capture spatial and temporal characteristics of actions at each individual view.
    % We then build a common space which tries to keep view-invariant features among views.
    % This is carried out by learning a set of linear transformations that project separated view features to common view space.
    % To this end, we first adopt Multi-view Discriminant Analysis (MvDA).
    % We then introduce pairwise-covariance of classes into the objective function that takes extra-class discriminance into account, namely pc-MvDA.
    % Both separated view features and common view features are integrated in an unified framework.
    % Two cross-view and multi-view evaluation protocols have been applied.
    % Experimental results on three datasets (IXMAS, MuHAVI, MICAGes) show that combination of ResNet-50 3D with proposed pc-MvDA gives the highest recognition accuracy in almost cases.
    % In addition, our proposed Pairwise-Covariance Discriminant Analysis (pc-MvDA) outperforms the original MvDA 5.29\% in average.
    % Cross-recognition results have been much better than the one without view discriminant analysis.
    % It demonstrates that practical deployment could be more feasible by using our proposed framework in reality.

\end{abstract}


% \begin{otherlanguage}{ngerman}
% %\renewcommand{\abstractname}{Kurzfassung}
% %\newcommand{\dtabstract}{\hyphenpenalty=10000}
% %{\dtabstract
% \begin{abstract}
% Das Gebiet des Music Information Retrieval befasst sich mit der automatischen Analyse von musikalischen Charakteristika. Ein Aspekt, der bisher kaum erforscht wurde, ist dabei der gesungene Text. Auf der anderen Seite werden in der automatischen Spracherkennung viele Methoden für die automatische Analyse von Sprache entwickelt, jedoch selten für Gesang. Die vorliegende Arbeit untersucht die Anwendung von Methoden aus der Spracherkennung auf Gesang und beschreibt mögliche Anpassungen. Zudem werden Wege zur praktischen Anwendung dieser Ansätze aufgezeigt. Fünf Themen werden dabei betrachtet: Phonemerkennung, Sprachenidentifikation, Schlagwortsuche, Text-zu-Gesangs-Alignment und Suche von Texten anhand von gesungenen Anfragen.\\
% Das größte Hindernis bei fast allen dieser Themen ist die Erkennung von Phonemen aus Gesangsaufnahmen. Herkömmliche, auf Sprache trainierte Modelle, bieten keine guten Ergebnisse für Gesang. Das Trainieren von Modellen auf Gesang ist schwierig, da kaum annotierte Daten verfügbar sind. Diese Arbeit zeigt zwei Ansätze auf, um solche Daten zu generieren. Für den ersten wurden Sprachaufnahmen künstlich gesangsähnlicher gemacht. Für den zweiten wurden Texte automatisch zu einem vorhandenen Gesangsdatensatz zugeordnet. Die neuen Datensätze wurden zum Trainieren neuer Modelle genutzt, welche deutliche Verbesserungen gegenüber sprachbasierten Modellen bieten.\\
% Auf diesen verbesserten akustischen Modellen aufbauend wurden Algorithmen aus der Spracherkennung für die verschiedenen Aufgaben angepasst, entweder durch das Verbessern der Robustheit gegenüber Gesangscharakteristika oder durch das Ausnutzen von hilfreichen Besonderheiten von Gesang. Beispiele für die verbesserte Robustheit sind der Einsatz von Keyword-Filler-HMMs für die Schlagwortsuche, ein i-Vector-Ansatz für die Sprachenidentifikation sowie eine Methode für das Alignment und die Textsuche, die stark schwankende Phonemdauern nicht bestraft. Die Besonderheiten von Gesang werden auf verschiedene Weisen genutzt: So z.B. in einem Ansatz für die Sprachenidentifikation, der lange Aufnahmen benötigt; in einer Methode für die Schlagwortsuche, die bekannte Phonemdauern in Gesang mit einbezieht; und in einem Algorithmus für das Alignment und die Textsuche, der bekannte Phonemkonfusionen verwertet.
% \end{abstract}
% \end{otherlanguage}
