% !TEX root = ../main.tex
\chapter{Technical Background} \label{chap:tech_background}

    \section{Introduction}\label{sec:background_introduction}
        In this chapter, a general description of the speech recognition process and its main components is introduced. 
        Next, the different methods of keyword spotting are highlighted and compared. 
        The objective is to choose the best technique for keyword spotting to use in the process of designing a robust DNN based keyword spotting engine. 
        Artificial Neural Networks (ANN) and their previous use in speech recognition is then described followed by more details on DNN. 
        This chapter, concludes with a presentation of various speech enhancement approaches found in the literature.
    
    \section{Keyword Spotting}\label{sec:kws}

        The general procedure for the tasks in this work is shown in figures \ref{fig:process_training} and \ref{fig:process_classification}. 
        Using data sets of audio (speech) and matching annotations, models are trained as outlined in figure \ref{fig:process_training}. 
        Then, these models are used to classify unseen audio data in order to generate annotations for them (figure \ref{fig:process_classification}).

        In detail, the necessary steps are:
        \begin{description}
            \item[Pre-processing] For the tasks in this work, pre-processing of the audio data is relatively straightforward and consists of normalization of the signal and averaging to a mono channel. 
            Additionally, the audio is usually downsampled to 16kHz because this is the lowest sampling frequency in most of the data sets, and downsampling is necessary for compatibility. 
            The language identification data sets are an exception; they are downsampled to 8kHz (more detail on the data sets is given in chapter \ref{chap:datasets}).
            In a system for polyphonic music, pre-processing could also include steps for source separation or Vocal Activity Detection (VAD). 

            \item[Feature extraction] The audio signal contains a lot of data that is redundant and irrelevant to the tasks. 
            For this reason, many types of so-called feature representations were developed over the years. 
            The features employed in this work are described in the next section.

            \item[Model training] Using both the audio features and the available annotations, models are trained with machine learning algorithms to gain an implicit understanding of the requested annotations (i.e. classes). 
            In this work, only supervised learning was employed. 
            The machine learning methods are described in section \ref{sec:dnn}.

            \item[Classification] The trained models can then be used on features extracted from unseen audio data to obtain their annotations (= classes).

            \item[Post-processing] In many tasks, the classification results are not used directly, but processed further. 
            In tasks like alignment and retrieval, for example, phoneme probabilities are matched with symbolic phoneme annotations. 
            In these cases, distance calculation methods as described in section \ref{sec:tech_distances} are required.
        \end{description}

        \begin{figure}
            \begin{center}
                \includegraphics[width=1\textwidth]{figures/process_training.png}
                \caption{Schematic of the training procedure of the considered tasks.}
                \label{fig:process_training}
            \end{center}
        \end{figure}

        \begin{figure}
            \begin{center}
                \includegraphics[width=1\textwidth]{figures/process_classification.png}
                \caption{Schematic of the classification procedure of the considered tasks (the model is the one created during training - see figure \ref{fig:process_training}).}
                \label{fig:process_classification}
            \end{center}
        \end{figure}

    \section{Feature Extraction}
        This section describes the Mel-Frequency Cepstral Coefficients (MFCCs) features. 
        MFCCs are among the most frequently used audio features in speech recognition. 
        They were first introduced in 1976 by Mermelstein and Davis \cite{mermelstein}\cite{davis_mermelstein} based on previous experiments by Bridle and Brown \cite{bridle_brown}.

        The basic idea behind MFCCs comes from experiments in human auditory perception. 
        Audio signals are transformed into a representation that is based on human perceptual sensitivities. 
        This is done by taking the Short-Term Fourier Transform (STFT) of an audio signal and then mapping the resulting spectrum from a linear frequency scale onto a Mel scale. 
        Then, the Discrete Cosine Transform (DCT) of the resulting log energies is calculated along the frequency axis to decorrelate the spectral band signals. 
        The result is a representation of the various frequencies within the spectrum (i.e. the cepstrum), which can be interpreted as ranging from the spectral envelope to the more fine-grained components. 
        In theory, this makes the result largely independent of the absolute frequencies (i.e. the pitch), but representative of the perceptual content (e.g. phonemes). 
        One point of criticism, however, is the lack of interpretability of the MFC coefficients.
        
        In detail, the calculation is performed as follows:

        \begin{description}
            \item[Short-term Fourier transform (STFT)] After cutting a signal $s$ into frames $s_i, i = 0,1,...,I$ (e.g. of 10ms duration) and windowing it (e.g. with a Hamming window), the Discrete Fourier Transform (DFT) is calculated for each time frame:
            \begin{equation}
                S_i(k) = \sum_{n=0}^{N-1} s_i(n)h(n)e^{-j 2\pi kn/N}, k = 0,1,...,K-1
            \end{equation}
            where $h(n)$ is the window of length $N$, and $K$ is the DFT length. For the further calculations, only the power spectrum is used:
            
            \begin{equation}
                P_i(k) = \frac{1}{N} \mid S_i(k) \mid ^2
            \end{equation}

            \begin{figure}
                \begin{center}
                    \includegraphics[width=0.6\textwidth]{figures/mel_filterbanks.png}
                    \caption{Example of a Mel filterbank. \cite{davis_mermelstein}}
                    \label{fig:mel_filterbanks}
                \end{center}
            \end{figure}

            \item[Mel-spectrum calculation] The resulting energies are mapped from the linear frequency scale to a perceptually motivated Mel scale. 
            This is done by convolving the spectrum with a set of $M$ triangular Mel-spaced filters $H_m(k)$, such as the ones shown in figure \ref{fig:mel_filterbanks}. 
            Furthermore, the resulting energy outputs are logarithmized:
            \begin{equation}
                X_m = \log_{10} \left( \sum_{k=0}^{K-1} \mid P_i(k) \mid \cdot H_m(k) \right), m=1,2,...,M
            \end{equation}

            \item[Discrete Cosine Transform (DCT)] Finally, the Mel-scale spectrum is transformed with a DCT, resulting in the so-called cepstrum:
            \begin{equation}
                C_j = \sum_{m=1}^{M} X_m \cdot \cos \left( (j+1) \cdot (m-1/2) \cdot \frac{\pi}{M} \right), j=0,1,...,J-1
            \end{equation}
            The $J$ MFC coefficients are retained as features. 
            The 0th coefficient can be interpreted as the power over all frequency bands, and the 1st coefficient as the global energy balance between low and high frequencies \cite{oshaughnessy}.
        \end{description}

        In addition, deltas and double-deltas are calculated to capture information about the feature's trajectory:
        \begin{equation}
            \Delta(C(n)) = C(n) - C(n-1) 
        \end{equation}
        \begin{equation}
            \Delta\Delta(C(n)) = \Delta(C(n)) - \Delta(C(n-1))
        \end{equation}

    \section{Deep Neural Networks} \label{sec:dnn}
        In recent years, Deep Learning has become the standard for machine learning applications \cite{schmidhuber_dl}. 
        This section also describes a new approach that was used extensively in this work: Deep Neural Networks (DNNs).

        \subsection{Artificial Neural Networks}
            \paragraph{History} Artificial Neural Networks have a long research history. 
            Based on an idea by McCulloch and Pitts from 1943 \cite{mcculloch_pitts}, they were slowly developed into functional algorithms for classification and pattern recognition. 
            Rosenblatt proposed a hardware design for a single-layer perceptron in 1958 \cite{rosenblatt}, while the first multi-layer networks were introduced by Ivakhnenko and Lapa in 1965 \cite{ivakhnenko}. 
            In 1969, Minsky and Papert posited many practical limitations for Neural Networks \cite{minsky_papert}, which led to a decrease in interest.

            The introduction of the backpropagation algorithm solved some of these issues and increased the training speed of multilayer networks \cite{werbos}, leading to a wider usage in speech recognition \cite{waibel_hanazawa} and other fields, such as computer vision \cite{srinivas} and Natural Language Processing \cite{goldberg}. 
            However, other algorithms such as SVMs began to produce better results over time and thus overtook Neural Networks in popularity.

            Over time, processing speed of computers increased, and better strategies and hardware for parallel computing became available. 
            This made the training of networks with many more layers possible, allowing for a much better adaptation to high-dimensional problems \cite{hinton_osindero}. 
            Over the past 10 years, this so-called ``Deep Learning'' has become the state of the art for many machine learning problems \cite{nytimes}\cite{schmidhuber_dl}\cite{deng_hinton_kingsbury}.

            \paragraph{Functionality} Artifical Neural Networks (ANNs) are inspired by ``real'' Neural Networks - i.e. the human brain and nervous system. 
            They consist of neurons, which are nodes that can process inputs and send outputs, and the connections between them. 
            These neurons are grouped in layers: An input layer, an output layer, and a number of hidden layers in between them. 
            Historically, ANNs had no hidden layers at all; this type of ANN was called ``perceptron''. 
            Later, hidden layers were introduced and the resulting networks were called ``Multilayer Perceptrons'' (MLPs).
            
            Networks with no hidden layers are only able to solve linear problems; the introduction of hidden layers added non-linear projections to the calculation. 
            Recent ``Deep'' Neural Networks (DNNs) possess more hidden layers, leading to an exponential increase of the degrees of freedom and thus the possibility to model much more complex problems.

            \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.2\textwidth]{figures/neuron.png}
                \caption{Functionality of a single neuron in an ANN. The neuron computes a weighted sum of its inputs, and then applies an activation function, resulting in output $y$. \cite{mackay}}
                \label{fig:neuron}
            \end{figure}


            \begin{figure}
                \centering
                \begin{subfigure}[t]{0.3\textwidth}
                    \includegraphics[width=\textwidth]{figures/activation_heaviside.png}
                    \caption{Heaviside binary step}% activation function}
                    \label{fig:activation_heaviside}
                \end{subfigure}
                \begin{subfigure}[t]{0.3\textwidth}
                    \includegraphics[width=\textwidth]{figures/activation_sigmoid.png}
                    \caption{Sigmoid}% activation function}
                    \label{fig:activation_sigmoid}
                \end{subfigure}
                \begin{subfigure}[t]{0.3\textwidth}
                    \includegraphics[width=\textwidth]{figures/activation_relu.png}
                    \caption{ReLU}% activation function}
                    \label{fig:activation_relu}
                \end{subfigure}
                \caption{Activation functions used for ANN neurons. \cite{medium}}
            \end{figure}
                  
               
            The function of a single neuron is visualized in figure \ref{fig:neuron}. 
            Classic neurons compute a weighted sum $z$ of their inputs $x = (x_1,x_2,...,x_I)$:
            \begin{equation}
                z = w_0 + \sum_{i=1}^I x_i w_i = w_0 + w^\top x
            \end{equation}

            where $w = (w_1,w_2,...,w_I)$ is a vector of weights, and $w_0$ is a constant bias. 
            This result is often called the ``logit''. 
            Then, a nonlinear activation function can be applied. 
            In perceptrons, this was the Heaviside step function, shown in figure \ref{fig:activation_heaviside}, resulting in a binary output:
            \begin{equation}
                y = \begin{cases}
                    1 \text{ if } z \leq 0\\
                    0 \text{ otherwise}
                \end{cases}       
            \end{equation}

            An activation function commonly used is the sigmoid function, shown in figure \ref{fig:activation_sigmoid}:
            \begin{equation}
                y = \frac{1}{1 + e^{-z}}
            \end{equation}

            Neurons of this type are called ``logistic'' neurons. 
            This function is often applied because it generates a smooth, real-valued output and has easy-to-use derivatives, simplifying the training process.

            Rectified linear units (ReLUs) are also used frequently since they train faster than logistic units and retain more information that is relevant in the middle layers (in particular, this leads to sparsity for small inputs and a lower risk of vanishing or exploding gradients). 
            The function is shown in figure \ref{fig:activation_relu}:
            \begin{equation}
                y = \max(0, z)
            \end{equation}

            In the last layer, a so-called softmax activation function is often applied. 
            This function takes the outputs of all neurons into account, and computes a probability distribution (i.e. all the outputs will sum up to one and represent the likelihood of the corresponding class):
            \begin{equation}
                y_j = \frac{e^{z_j}}{\sum_{k=1}^M e^{z_k}}, 1 \leq j \leq M
            \end{equation}

            where $M$ is the number of output neurons.

            \paragraph{Training} Neural Network training is nowadays usually performed via the backpropagation algorithm. 
            This algorithm is based on the calculation of a cost (or error) function $E$, which computes the difference between the network output and the expected output (e.g. the annotated classes in the training data). 
            Then, the partial derivatives of the cost are calculated with regards to the weights, using the outputs and logits for the chain rule:
            \begin{equation}
                \frac{\partial E}{\partial w_{ij}} = \frac{\partial E}{\partial y_j} \frac{\partial y_j}{\partial z_j} \frac{\partial z_j}{\partial w_{ij}}
            \end{equation}

            Then, the weights are adjusted accordingly:
            \begin{equation}
                \Delta w_{ij} = - \eta \frac{\partial E}{\partial w_{ij}}
            \end{equation}

            where $\eta$ is the so-called learning rate, which must be chosen carefully to ensure convergence of the training process. 
            In the same way, the error is propagated further backwards through the model to adjust the weights of the previous layers. 
            This is done until all the weights have been adjusted. 
            Then, the next batch of training data is passed through the model and the process is repeated. 
            The training process is performed until the weights converge (or until a fixed number of iterations has been reached).

            A commonly used cost function is the squared error measure (employing the $L^2$ or Euclidean norm):
            \begin{equation}
                E_S = \frac{1}{2} \sum_{j=1}^{M} (t_j - y_j)^2
            \end{equation}

            where $t_j$ is the target value for output unit $j$. 
            Alternatively, the cross-entropy is often chosen as the cost function when using a softmax output layer:
            \begin{equation}
                E_C = - \sum_{j=1}^M t_j \log y_j
            \end{equation}

            \paragraph{Curse of dimensionality} One of the major issues in Neural Networks is that of overfitting: 
            The model adapts too well to the training data and is not able to sufficiently generalize to new data anymore. 
            This is especially relevant for deep networks because of the many degrees of freedom.

            There are several strategies to overcome this problem. One of the most frequently used regularization techniques is dropout: 
            During training, nodes in the network are deactivated randomly, effectively resulting in many different network structures and making the whole network robust to variations in the data. 
            The most crucial point, however, is the amount of data used for training the network. 
            The more variety is available, the lower the risk of overtraining.

            The so-called ``curse of dimensionality'' is a concept that applies to many machine learning algorithms. 
            However, Goodfellow et al. make an argument that deep models operate on a different level altogether \cite{goodfellow}: 
            Where traditional machine learning methods make an assumption of smoothness of the hidden functions to be represented, deep models actually attempt to model the underlying structures in a distributed way. 
            This means that information about outputs can be learned in a shared way - i.e., if two classes have something in common, the model is also able to represent this fact. 
            Practically, the many degrees of freedom do not lead to ``learning by heart'', but instead allow for an internal representation of highly complex relationships. 
            Goodfellow et al. mention two interpretations of the deep modeling capabilities: 
            Learning a representation composed of simpler representations (e.g. corners defined by edges), or learning sequential steps that build on top of each other (e.g. first locate objects, then segment them, then recognize them). 
            There is also a number of publications that demonstrate better generalization abilities for deeper networks \cite{bengio07}\cite{bengio09}\cite{ciresan}\cite{erhan09}\cite{krizhevsky}\cite{szegedy}. 
            In an experiment in \cite{goodfellow_experiment}, models with three hidden layers overfit at 20 million parameters, while a deep model (11 hidden layers) benefits from more than 60 million. 
            This is because a deep model has the capability to learn the actual explanatory factors behind the expected outputs (e.g. learning about different genders or whether a person is wearing glasses when modeling faces \cite{radford}).

            \paragraph{Types} In addition to the various types of neurons, ANNs themselves can be grouped into different types. 
            In their basic configuration, ANNs will have multiple layers of the described neurons where connections are only allowed in one direction. 
            A schematic is shown in figure \ref{fig:feedforward}. 
            This type of network is called ``feed forward'' or, in the context of Deep Learning, a Deep Neural Network (DNN). 
            In this work, DNNs with logistic units in the hidden layers and a softmax output layer were used in phoneme classification tasks.

            \begin{figure}[htbp]
                \centering
                \includegraphics[width=0.4\textwidth]{figures/feedforward.png}
                \caption{Schematic of a feed-forward neural network with an input layer with three neurons, two hidden layers with four neurons each, and an output layer with two neurons. \cite{medium}}
                \label{fig:feedforward}
            \end{figure}

            If connections that skip back are allowed in the network, the network is called a Recurrent Neural Network (RNN). 
            These networks are particularly useful for modeling time-dependent series because they have the ability to ``store'' temporal states in their units. 
            However, they were deemed impractical for a long time because they exhibit the exploding/vanishing gradient problem during training \cite{Hochreiter1998}. 
            This problem was solved with the introduction of memory cells in place of neurons, in particular Long Short-Term Memory (LSTM) units \cite{Hochreiter1997} and Gated Recurrent Units (GRU) \cite{Cho2014}. 
            Nowadays, RNNs are being used successfully in a number of research tasks \cite{lstm_sak_senior}.

            Another type of Neural Network are Convolutional Neural Networks (CNNs). 
            These networks add layers of filters (so-called convolutional layers) before or in between classic fully-connected layers. 
            The parameters of these filters are trained jointly with the other layers. For this reason, CNNs are able to ``learn'' a feature representation of the input. 
            They were first used in image recognition \cite{krizhevsky}, but are now also being used for audio-related tasks such as environmental sound classification \cite{piczak} and ASR \cite{abdelhamid}. 
            A disadvantage of both RNNs and CNNs is the computational complexity of training them, and the requirement for even more training data because they possess even more degrees of freedom than DNNs of comparable sizes.

        \subsection{Convolutional Neural Networks}

            The neural network or multi-layered network of neurons can learn the relation between the input and output using non-linear mapping function. 
            But in the case of images, a neural network would, for a given task, take into account the entire image. 
            This, in turn, makes the neural network highly inefficient in terms of pragmatic quality as well as neural network quality, as, the number of parameters to learn increases. 
            Hence, to solve this problem, another type of neural network called Convolution Neural Networks is used, which is specifically designed for computer vision-related tasks such as image classification, semantic segmentation, object detection, etc.

            The inception of CNNs happened in the year 1960 by D.H Hubel and T.N Wiesel in their paper \cite{hubel1962receptive} where they described two types of cells in the human brain (specifically in the visual cortex), simple cells, and complex cells. 
            Simple cells are activated when they identify basic shapes in a fixed area and a definite angle. 
            The complex cells have bigger receptive fields, and their output is not sensitive to specific positions in the field. 
            Taking inspiration from \cite{hubel1962receptive}, in the year 1998, CNNs was re-introduced in the paper \cite{lecun1998gradient} called LeNet-5 which was able to classify digits from hand-written numbers. 
            Since it's inception to the present era, the CNNs has been used in various state of the art applications, especially in the field of computer vision.

            There are four main operations in CNNs. 
            Figure \ref{fig:convolution_layer} shows how all the components are linked to each other in a CNN.
            These operations are the essential building blocks in every CNNs:
            \begin{itemize}
                \item Convolution operation
                \item Activation
                \item Pooling
                \item Fully-connected layer
            \end{itemize}

            \begin{figure}[h!]
                \centering
                    \includegraphics[width=\linewidth]{figures/convolution_layer.png}
                \caption{Components of a CNN}
                \label{fig:convolution_layer}
            \end{figure}

            \begin{figure}[h!]
                \centering
                    \includegraphics[width=0.8\linewidth]{figures/convolution_operation.png}
                \caption{A convolution operation \cite{VoPa}}
                \label{fig:convolution_operation}
            \end{figure}

            \paragraph{Convolution operation} Consider the example on figure \ref{fig:convolution_operation} showing how the convolution works. 
            There is an image (x) of \textit{7$\times$7} and a kernel or filter (k) of \textit{3$\times$3}.
            Slide the filter over the image pixel-by-pixel and for every position and compute the element-wise multiplication between the two matrices and then, add the outputs to get a final integer which forms a single element in the output matrix. 
            The output matrix is so-called an activation map or feature map. 
            Hence, a CNN investigates only a piece of the image rather than the entire image at once, making it different from other neural networks. 

            A convolution neural network learns the values (weights) of the filters on its own during the training process. 
            CNNs can have multiple numbers of filters such that more features from the image get extracted, and better the network becomes in observing and recognizing patterns. 
            
            The size of the feature map can be controlled by three parameters, they are:
            \begin{itemize}
                \item Depth - This is the number of filters we use for the convolution operation. 
                For example, if we use three distinct filters on the input image, we will have three feature maps that are stacked together. 
                
                \item  Stride - It determines the number of pixels by which the filter slides over the input matrix. 
                For example, if the stride value is 1, we move the filter over the input by one pixel at a time. 
                If the stride value is 3, we move the filter over the input by jumping 3 pixels at a time.
                
                \item Zero-padding - We pad the input matrix with zeros around the border such that filter can be applied to the bordering elements of the input matrix. 
                Applying zero-padding is called as wide-convolution and not using zero-padding is called as narrow-convolution.
            \end{itemize}

            \paragraph{Activation} The non-linear activation functions are responsible for the model to create complex mappings between the input and the output such that modeling complex data such as images, videos, audios, which are, high dimensional in nature is possible. 

            \paragraph{Pooling} Pooling step is done to reduce the dimensionality of the feature map and preserve the most important features from the map. 
            This process is also called as downsampling of upsampling.
            Pooling has further advantages like:
            \begin{itemize}
                \item It makes the input representations smaller and hence, more manageable.
                \item It reduces the number of parameters and therefore, controls overfitting.
                \item It makes the model unvaried to small distortions, translations in the input image.
                \item It makes an invariant representation of the image due to which, the network can detect objects at any located position in the image. 
            \end{itemize}

            There are three types of pooling: 
            \begin{itemize}
                \item Max pooling - Define a spatial neighbourhood $2\times2$ window and take the largest element from each window of the feature map to form a new output matrix. 
                Figure \ref{fig:maxpool} shows how max-pooling works. 
                
                \item Average pooling - Define a spatial neighbourhood $2\times2$ window and take the average of all the elements in each window from the feature map, to form a new output matrix with the average values.
                
                \item Sum pooling - Define a spatial neighbourhood $2\times2$ window and take the sum of all the elements in the window from the feature map, to form the new output matrix with the sum values.
            \end{itemize}
            
            \begin{figure}[h!]
                \centering
                    \includegraphics[width=0.4\linewidth]{figures/maxpooling.png}
                \caption{Max-pooling operation}
                \label{fig:maxpool}
            \end{figure}

            \paragraph{Fully connected layer} The term 'fully connected' implies that every neuron in the layer is connected with the neurons of the previous layer. 
            The convolutional layers and pooling layers are used to extract features from the input image, while the fully connected layer uses these extracted features to classify the input image into separate classes based on the training dataset. 
            A softmax function is used as the activation function in order to get the probabilities of each class in the dataset. 

        \subsection{Recurrent Neural Networks}

            A recurrent neural network (RNN) is rather similar to a multilayer perceptron, but it takes previous inputs into account. 
            The input to an RNN is a sequence of samples. 
            An RNN can be used for other tasks where the order of samples are important, e.g. time series forecasting or natural language processing.
            Recurrent neural networks stores hidden representations of inputs and conveying that information to the next input in the sequence \cite{Goodfellow-et-al-2016}. 

            \begin{figure}[t]
                \centering
                \includegraphics[width=0.5\textwidth]{figures/simple_rnn.png}
                \caption{General graph of a sequence-to-sequence RNN unfolded through time, where a sequence of inputs $x$ predicts a sequence of outputs $y$. $\mathbf{U}$, $\mathbf{V}$ and $\mathbf{W}$ are weights that are shared throughout the sequence, independent of the position.}
                \label{fig:rnn}
            \end{figure}

            As seen in figure~\ref{fig:rnn}, an RNN can be seen as a feed-forward neural network where previous time steps are used as input. 
            Assuming a hidden layer activation function $\sigma_h$ and an output activation function $\sigma_o$, the recurrent nature of the model can be further explained. 
            Let the other notation be as in figure~\ref{fig:rnn}. 
            \begin{equation}
                \hat{y_t} = \sigma_o(V \cdot h^{(t)}) = \sigma_o(V \cdot  \sigma_h(U x^{(t)} + W h^{(t-1)}))
            \end{equation}

            \begin{equation}
                h^{(t-1)} = \sigma_h(Ux^{(t-1)} + W h^{(t-2)})
            \end{equation}


            Clearly, the prediction $\hat{y}^{(t)}$ is dependent on the previous inputs $x^{(i)}$ and hidden states $h^{(i)}$. 
            The model structure is similar for sequence-to-one classifications, i.e. a sequence $(x_i^{(1)}, x_i^{(2)}, ..., x_i^{(l)})$ predicts one label $\hat{y}_i$. 
            Given a sequence length $l$, only the final step $x_i^{(l)}$ provides an output $y_i$. 
            This output is however influenced by the previous inputs $x_i^{(t)}$ through the hidden layers $h^{(t)}$ and the weights $W$, see Figure~\ref{fig:rnns2o}. 

            \begin{figure}[t]
                \centering
                \includegraphics[width=0.5\textwidth]{figures/rnn-s2o.png}
                \caption{Schematic graph of a sequence-to-one RNN unfolded through time, where a sequence of inputs $(x^{(1)}, ..., x^{(l)})$ predicts one output $y$.}
                \label{fig:rnns2o}
            \end{figure}

            Even though the general idea of the previously described method is widely used, the model is not used precisely as described. 
            The simple RNN-structure suffers from the \textit{vanishing gradient problem}. 
            To understand this problem, one must understand how the network is trained, which is outside the scope of this report. 
            In short, long term dependencies are too many time-steps away to have an impact when the network is trained. 

            In practice, either Long-Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) are used instead. 
            The main difference is that information that is deemed important is allowed to pass on to later time-steps without too much interference from hidden several dot products and activation functions. 
            This alleviates the vanishing gradient problem, even though the core idea is the same as the previously described simple RNN \cite{chollet2017deep}.


            \paragraph{Long Short-Term Memory (LSTM)} The LSTM architecture, contrary to regular RNNs, has an additional hidden state that is never directly outputted (see Figure~\ref{fig:lstm_cell}). 
            This additional hidden state can then be used by the network solely for remembering previous relevant information. 
            Instead of having to share its ``memory'' with its output, these values are now separate. 
            During the training process, an LSTM learns what should be remembered for the future and what should be forgotten, which is achieved by using its internal weights.

            \begin{figure}[h!]
                \begin{center}
                    \includegraphics[scale=0.5]{figures/lstm_cell.png}
                \end{center}
                \caption{A single LSTM cell. From~\cite{olah2015understanding}\label{fig:lstm_cell}}
            \end{figure}

            As can be seen in the figure, there are quite a few more parameters in this cell than in a normal RNN cell. 
            The calculation of the output vector and the hidden vector involves several operations, a full explanation of which can be found in~\cite{olah2015understanding}. 
            First of all the network determines how much of the hidden state to forget, also called the forget gate. 
            This is done by pushing both the previous iteration's output (\(c_{t-1}\)) and the forget gate vector (\(f_t\)) through a matrix multiplication. 
            This allows the network to forget values at specific indices in the previous iteration's output vector. 
            \(f_t\) can be obtained by using formula~\ref{eq:forget_vector_lstm}, where \(W\) contains the weights for the input and \(U\) contains the weights for the previous iteration's output vector, \(x_t\) refers to the input, \(h_{t-1}\) to the previous iteration's output vector and \(b\) to a set of bias vectors:
            \begin{equation} \label{eq:forget_vector_lstm}
                f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
            \end{equation}

            The network then determines what to remember from the input vector.
            This is commonly referred to as the input gate. 
            This is done by pushing the previous forget gate's output as well as the input gate through a matrix addition function. 
            The output of the input gate (\(i_t\)) can be found by using the following formula:
            \begin{equation} \label{eq:input_vector_lstm}
                i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
            \end{equation}

            The final hidden state vector (\(c_t\)) can then be found by using the previous two results as follows, where \(\circ \) denotes the Hadamard product (where each value at index \(ij\) is the product of the values at the indices \(ij\) in the two input matrices): 
            \begin{equation} \label{eq:hidden_state_vector_lstm}
                c_t = f_t \circ c_{t-1} + i_t \circ \sigma(W_c x_t + U_c h_{t-1} + b_c)
            \end{equation}

            This vector is then passed on to the next iteration. 
            Now the output gate vector \(o_t\) is calculated:
            \begin{equation} \label{eq:output_gate_lstm}
                o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
            \end{equation}

            The output state \(h_t\) can then be obtained:
            \begin{equation} \label{eq:hidden_output_gate_lstm}
                h_t = o_t \circ \sigma(c_t)
            \end{equation}

            This results in a version of an RNN that is able to remember more and is more liberal in choosing what information it wants to keep in the hidden state and what it wants to discard. 
            This makes LSTM networks better suited for tasks involving series of data. 
            This has lead to the LSTM architecture becoming the dominant RNN architecture. 

            \paragraph{Gated Recurrent Units (GRUs)} Another RNN architecture is the Gated Recurrent Unit (GRU), introduced in~\cite{cho2014learning}. 
            This architecture combines the input and forget gates into a single so-called ``update gate'' and also merges the cell state and hidden state (see Figure~\ref{fig:gru_cell}). 
            The calculation of the merged output vector once again consists of several operations. The network first computes the ``reset gate'' \(r_t\) using the following function, where \(W_r\) are the weights for the reset gate and \([h_{t-1}, x_t]\) signifies the concatenation of \(h_{t-1}\) and \(x_t\):

            \begin{figure}[h!]
                \begin{center}
                    \includegraphics[scale=0.5]{figures/gru_cell.png}
                \end{center}
                \caption{A single GRU variation cell. From~\cite{olah2015understanding}\label{fig:gru_cell}}
            \end{figure}

            \begin{equation} \label{eq:gru_reset_gate}
                r_t = \sigma(W_r [h_{t-1}, x_t])
            \end{equation}

            After this, the ``update gate'' \(z_t\) is computed as follows, where \(W_z\) holds the weights of the update gate:
            \begin{equation} \label{eq:gru_update_gate}
                z_t = \sigma(W_z [h_{t-1}, x_t])
            \end{equation}

            The output vector \(h_t\) (representing both the cell's output and its state) can then be computed by the following function:
            \begin{equation} \label{eq:gru_output}
                h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h_t}
            \end{equation}

            Where \(\tilde{h_t}\) is computed by:
            \begin{equation} \label{eq:gru_output_partial}
                \tilde{h_t} = \tanh(W * [r_t * h_{t-1}, x_t])
            \end{equation}

            \paragraph{Bidirectional RNNs} It should be mentioned that when RNNs are used, they are often wrapped with a bidirectional layer.
            This simply reverts the input sequence and enters the sequence in both the original and the reverse direction to two separate RNNs, usually LSTM or GRU. 
            The usefulness of this is particularly intuitive when looking at the network in Figure~\ref{fig:rnn}. 
            When processing the entry $x^{(j)}$, only the entries $t < j$ are known. 
            However, tokens later in the sequence might have an impact on the previous outputs of the model. 
            Bidirectional RNNs can catch patterns that are overlooked by regular RNNs.
